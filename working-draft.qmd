---
title: "Scientific Validity of Firearms and Toolmark Examination"
bibliography: refs.bib
classoption: table
header-includes: |
  \usepackage{booktabs}
  \usepackage[multiple]{footmisc}
  \usepackage{etoolbox}
    
  \makeatletter
  % Argument: custom value to use locally for \floatingpenalty inside footnotes
  \newcommand*{\mySpecialfootnotes}[1]{%
    \patchcmd{\@footnotetext}{\floatingpenalty\@MM}{\floatingpenalty#1\relax}%
             {}{\errmessage{Couldn't patch \string\@footnotetext}}%
  }
  \makeatother
format:
  pdf:
    include-in-header: "preamble.tex"
    keep-tex: true
    number-sections: true
    pdf-engine: xelatex
geometry:
  - left=1in
  - right=1in
  - top=1in
  - bottom=1in
biblio-style: abbrvnat
csl: bluebook-law-review.csl
---

# Statement of Interest of Amicus Curiae

Amicus curiae are professors and researchers in scientific disciplines who are concerned with the use of scientific studies to support the reliability of forensic evidence in the legal system.
Most of us are not involved in the study of forensic disciplines directly, but we are scientists, statisticians, and researchers who are qualified to assess research design, execution, and the claims which are made as a result of research studies in firearms and toolmark analysis.
We speak for ourselves, as private parties, and not for our institutions.

# Introduction

This amicus brief outlines the fundamental research principles used to evaluate the scientific validity of a method.
What is discussed in this brief is not new; it describes the research requirements adhered to in science-based fields.
The brief then discusses the application of these principles to the method used by firearms and toolmark examiners.
Adhering to the principles of sound research design and statistical analysis is fundamental to any applied science.
There is no exception for forensic science.
While the firearms and toolmark field has made strides, current research does not yet support the claims made by the discipline.
Specifically, existing research studies that evaluated accuracy, reliability, and reproducibility of firearms examination have substantial flaws, described below.
Our conclusion is that firearms examination has not been demonstrated to be accurate, reliable, or reproducible.
Error rates for firearms examination (e.g., false positive identifications) are currently unknown, since existing studies are inadequate to establish these.
Issues with experimental design, participant selection, statistical analysis, and the interpretation of estimates pervade the current validation studies.
As just one example, studies count inconclusive responses–those in which the examiner cannot make a definitive conclusion–as effectively correct (i.e., not as errors), which results in misleadingly low reported error rates^[Inconclusive responses are included in the total number of comparisons performed, but not included as errorso in the numerator.].
Treating inconclusive responses as effectively correct results in reported error rates as low as zero percent.
If inconclusives are instead treated as errors, error rates can be as high as 93%.
The true error rate is likely between these two extremes, but until more well-designed research is performed, it remains unknown.
While there are encouraging developments in research design, data from a recent study shows an alarming lack of consistency in decisions when the same examiner was presented with the same evidence twice, and when different examiners were presented with the same evidence[@bajicValidationStudyAccuracy2020]. 
These new data further undermine the claim of a well-developed, scientifically valid method and cannot go unaddressed.


# Argument

## Relevant Expertise

If the Court wishes to understand how weapons leave marks on fired ammunition, or how an examiner performs a comparison between two bullets or cartridge cases, practitioners are the best group to consult.  
If, however, the determination the Court must make is not about how the forensic process is performed, but rather how well it is performed - or, in the ideal, how well it can be performed – the person to consult is the research scientist.

An analogy to the field of medicine, a field with similarly high stakes consequences, is helpful.  
The epidemiologist who researches disease has a very different role and skill set than the doctor who treats patients [^interested-party]. 
If one wants to know about the effects of a disease on a population or how to slow the spread of an emerging virus, it is far more effective to consult the epidemiologist; if one wants to know how to treat a patient afflicted, then the doctor is the appropriate expert to consult. 

[^interested-party]: Another relevant and important distinction is between interested and disinterested parties. Those with a financial or personal stake in the outcome are generally not the only people who should be tasked with researching a particular issue.

## Scientific Validity

The basic requirements of any valid scientific method are that it must be:
- accurate, meaning the conclusion reached is the correct one,
- repeatable, meaning the examiner reaches the same conclusion when presented with the same evidence, 
- reproducible, meaning different examiners reach the same conclusions when analyzing the same evidence.

A valid method or instrument gives consistent results.  
A scale, for example, can be perfectly reliable and report the same weight for the same object each time it’s weighed.  
That does not mean the scale is accurate; it may consistently report the wrong weight.  
Reliability, or consistency, is a necessary component of a scientifically valid method, but does not, on its own, establish scientific validity.
A valid method produces accurate results.  
It is not possible to assess the accuracy of a method without testing it on samples where ground truth is known, meaning testing using samples of known origin. 
Because ground truth is unknown in case work – the examiner does not know if the two bullets were fired in the same gun or different guns – case work cannot serve to support the validity of the method, even when a second examiner agrees with the first examiner.

The goal of any validation study is to understand the range of conditions under which the method works as required, how well it performs, and to identify conditions under which it is likely to fail. 
A high quality study design is needed in order to achieve these goals.
Evaluating the validity of an entire discipline requires many studies, over a range of conditions, with some replication; in addition, studies used to support the validity of a discipline must be well-designed, using appropriate test problems, instructions, sampling procedures, and statistical practices when analyzing the results. 
Scientifically, supporting studies should meet several conditions: they should be designed in consultation with statisticians, published in scholarly journals that require peer review by statisticians and subject matter experts[^peer-review], the results must hold up over time and replication[^poly-water], and the studies must be conducted over a wide range of conditions that are representative of those seen in applied settings. 
As an analogy, consider what is required for regulators such as the U.S. Food and Drug Administration (FDA) to approve a new drug. 
Multiple, high quality randomized trials are required, each of which needs to demonstrate efficacy of the drug for the target population. 
For firearms examination, though there have been randomized experiments, even the best ones have significant flaws.
Moreover, even if we look past the flaws, evidence is beginning to accumulate that firearms and toolmark examination is neither repeatable nor reproducible, and thus, is not scientifically valid.

[^peer-review]: Trade journals, including the AFTE Journal, are sometimes peer reviewed, but the peers are practitioners rather than research scientists; these reviews focus on the forensic procedures but neglect to consider the design of the study and the statistical validity of any reported results. As a result, studies from these journals often have serious methodological flaws. Research journals are not immune from this problem, but it is at least more likely that reviewers who are active research scientists and have training in statistical analysis and experimental design.

[^poly-water]: Note that even prestigious scientific journals and respected institutions have published scientific results which do not hold up to the test of time. @lippincottPolywaterVibrationalSpectra1969 and other follow-up papers demonstrated unique properties of a form of water called polywater, first discovered in the USSR and then replicated in US labs and at the National Bureau of Standards (now known as NIST). These properties were later shown to be identical to those of sweat, @rousseauPolywaterSweatSimilarities1971, suggesting that the original documented and peer-reviewed phenomenon was a result of replication of conditions producing laboratory contamination of samples. More details can be found in @strombergCuriousCasePolywater2013. 



## Firearms and Toolmark Examination

Firearms examiners compare two bullets or two cartridge cases under a comparison microscope.  Historically, the Association of Firearms and Tool Mark Examiners (AFTE) method permitted an examiner to render three subjective judgments – identification, meaning they were fired in the same gun, exclusion, meaning they were fired from different guns, and inconclusive. The AFTE Range of Conclusions also includes three inconclusive options^[AFTE Theory of Identification and AFTE Range of Conclusions, https://nij.ojp.gov/nij-hosted-online-training-courses/firearms-examiner-training/module-09/afte-theory-identification.] - 

a. Agreement of all discernible class characteristics and some agreement of individual characteristics, but insufficient for an identification. 

b. Agreement of all discernible class characteristics without agreement or disagreement of individual characteristics due to an absence, insufficiency, or lack of reproducibility. 

c. Agreement of all discernable class characteristics and disagreement of individual characteristics, but insufficient for an elimination.

There are many ways to attempt to quantify how often judgments are wrong, and it is important to fully understand the strengths and weaknesses of each potential approach. 
In the field of medicine, for example, the National Institutes of Health (NIH) has very strict requirements that ensure that the design of validation studies meet the highest standards, and the Food and Drug Administration regulates which tests can be used on patients[@nationalinstitutesofhealthInclusionWomenMinorities2022]
There is currently no similar oversight mandating appropriately designed studies in the field of firearms and toolmark analysis.
As such, the fact that a study was performed, or even published, does not mean that the results are reliable.
One has to evaluate the design of the studies to determine whether they meaningfully contribute to the overall scientific validity of the discipline.



## Stages of Scientific Support for Firearms and Toolmark Analysis

Examiners did not always claim to be able to identify a specific fired bullet to a specific gun.  
At the inception of firearms examination as a discipline, examiners made claims supported by their individual experience, borne of an understanding of the mechanics of firearms and the (relatively new) ability to accurately measure minute details of the firearm and ammunition [@hallMissileWeapon1900].
These claims were supported by descriptive data, in that there were measurements being made in a laboratory setting, but examiners did not make source identification decisions nor establish any systematic data collection that would allow for inference that two bullets or cartridge cases were fired by the same gun. 

By the 1930s Valentine’s Day Massacre, however, examiners began to make claims about the individualizing nature of the firearms manufacturing process [@goddardValentineDayMassacre1930]. 
These claims were still unsupported by any systematic data collection, but the claims were more expansive than previous written records, which highlight descriptive characteristics and do not attempt to draw a direct connection between fired ammunition and a specific weapon. 
Examiners had moved on to inferential claims, where the accumulated “data” of their past experiences were used to support more general claims about the methodology used in firearms and toolmark identification.
Over the next 60 years, the field focused on research into the investigative method and procedures, with some forays into initial attempts at quantitative evaluation methods. 
The next development of interest to the Court addressed the question of examiners’ ability to apply a procedure to evaluate a set of samples of known provenance and come up with the correct answer.
Such “black-box” studies are so called because they treat the examiner and evaluation procedure as an unobservable entity and evaluate only the resulting answer (rather than assessing the reasoning behind it). 
The subjective, visual comparisons performed during examiner evaluations cannot be tested step-by-step, a marked difference from disciplines like DNA where each step of a lab test can be audited separately.

One of the first studies to attempt to test examiners’ ability to reach the correct conclusion was @brundageIdentificationConsecutivelyRifled1994, which served as a model for error rate studies in firearms and toolmark analysis for the next 15-20 years, with updated data published as recently as 2019 [@hambyWorldwideStudyBullets2019].
Unfortunately, the design of the Brundage-Hamby studies is *deeply flawed*. 
As a result, the re-use of this study design has resulted in a collection of studies which cannot be relied upon for calculation of an error rate. 
These studies have two separate but related design flaws which, on their own, render the results unhelpful in understanding the performance of the method: they use multiple unknown and known samples in the same kit, and they are “closed-set” studies, meaning examiners know that all unknown samples have a matching known source[^other-flaws].

[^other-flaws]: The studies suffer from other design flaws as well, as discussed more fully below.

When multiple unknown and known samples are included in the same kit, examiners do not list out all comparisons which were performed. 
Instead, they fill in only the matching known sample for each unknown. 
This does not allow us to calculate the error rate for a comparison, because we do not know how many comparisons were performed[^closed-set-explanation].
As a result, it is impossible to estimate the probability of a missed elimination (where an examiner fails to eliminate samples from different sources). 
In addition, due to the knowledge that all unknown samples match a provided known, examiners can select the closest known sample instead of making a positive identification based on the visible evidence. 
All told, this leads to a misidentification rate that we can expect to be lower than in case work[^inconclusive-set-design].
While these studies also have other issues (e.g. sampling bias), the structural flaws of the study are severe enough on their own to render the results unusable for evaluating examiners ability to reach the correct conclusion. 

[^closed-set-explanation]: To illustrate why a closed set test prevents the researcher from knowing the number of comparisons conducted, consider the case when there are two unknowns (A and B) and two knowns (C and D). One examiner might compare each of the unknowns to each of the knowns (A-C, A-D, B-C, B-D) for a total of four comparisons. Another examiner, however, might first compare A to C and determine them a match, and therefore refrain from comparing A to D. Accounting for all the possibilities, there could be anywhere from 2 to 4 comparisons. As the number of unknowns and knowns grow, the range of possibilities also increases. For example, if there were four knowns and four unknowns, the possible number of comparisons completed can range from 4 to 16.

[^inconclusive-set-design]: The issue of inconclusive responses, which figures so prominently in better designed open studies, does not typically arise in "closed set studies," in part because the additional information that all unknown items share a source with known items presented as part of the set is used by examiners when making their comparisons. 

Many studies which followed Brundage (1994) emulated the multiple-known to multiple-unknown study design, precluding a determination of the number of comparisons which is essential information for an error rate study, though not all of these studies were also closed-set studies. 
In 2014, the Ames Laboratory undertook a study in conjunction with the Department of Defense.  Recognizing the confounding problem of the previous studies,[^ames-quote] the researchers modified the test problem design so that the number of comparisons could be calculated.  
Similar designs were also adopted by @keislerIsolatedPairsResearch2018 and @chapnickResults3DVirtual2021. 
While these studies have better test problem design (e.g. open v. closed), they still have some major flaws common to almost all studies in firearms and toolmark examination: 
there are significant levels of participant drop-out which are not accounted for in the analysis of results[^participant-drop-out],
participants self-select instead of being randomly selected as part of a representative sample[^representative-sample], and there is no objective assessment of the difficulty of comparisons in each study (which makes it difficult to compare studies or assess the relevance of a study to a specific case). 
The treatment of inconclusive responses is also a significant issue discussed below.

[^ames-quote]: "[T]he design of these previous studies, whether intended to measure error rates or not, did not include truly independent sample sets that would allow the unbiased determination of false-positive or false-negative error rates from the data in those studies." @baldwinStudyFalsePositiveFalseNegative2014.

[^participant-drop-out]: Participant drop-out is of particular concern because in many cases it occurs after participants have seen the study materials. If the materials are difficult comparisons, then less-skilled or less-confident examiners may drop out because they do not want to increase the published error rates for the discipline. Of course, there are other reasons participants may drop out, such as casework overloads, but the fact that there are explanations for the drop-out rate that would be related to the calculated error rate make the estimates generated from these studies statistically questionable. That the researchers do not account for these issues when calculating possible error rates (as is common in other disciplines with participant drop-out, such as medicine) is much more problematic.

[^representative-sample]: Most scientific studies involving humans take place on volunteer samples. What is problematic in FTE studies is that researchers make no effort to ensure that the participants in the study accurately reflect characteristics of the active examiner population, such as experience, lab type, training, and education level. Again, medical studies are a good comparison group: participants in pharmaceutical trials are also volunteers, but substantial effort is devoted to try to enroll participants who are representative of the general population, in accordance with guidelines from the NIH. Without a representative sample, it is difficult to justify generalizing the results of the study to the wider population – a critical step for utilizing these studies in a legal setting. @nationalinstitutesofhealthInclusionWomenMinorities2022

As the discipline of firearms and toolmark analysis has matured, and as pressure to validate the conclusions made by examiners using scientific studies of the examination process has increased, more sophisticated study designs have been developed which provide more nuanced ways to assess the discipline than raw error rates.
The most recent studies to be released examine not only error rate, but also the repeatability and reproducibility of examiner conclusions when assessing both bullets and cartridge cases. 
The first study, colloquially known as Ames II, was published across a suite of papers examining, in turn, the design, reliability results, repeatability, and reproducibility of firearms comparisons.  [@bajicValidationStudyAccuracy2020;@monsonPlanningDesignLogistics2022;@baldwinStudyExaminerAccuracy2023;@monsonAccuracyComparisonDecisions2023;@monson2023repeatability] [^ames-ii-withdrawn].
The second study, @hicklinAccuracyReproducibilityBullet2024, was completed in collaboration with scientists at NIST, and examines the effects of different ammunition, polygonal rifling, and other factors on reliability, repeatability, and reproducibility. 
While these studies still have many of the same flaws identified in other modern studies, these designs are much improved from the closed-set studies which dominated this field previously, and demonstrate that it is possible in the future to design studies which directly answer questions of interest to the Court: 
is firearms analysis repeatable and reproducible? 
Do the method’s error rates support its conclusions?

[^ames-ii-withdrawn]: Bajic et al., Validation Study of the Accuracy, Repeatability, and Reproducibility of Firearm Comparisons, 127 (2020), https://www.scribd.com/document/586448513/Ames-FBI-Validation-Study. We reference the full 127 page report of the Ames Laboratory to the FBI, comprehensively detailing data and analysis estimating accuracy, repeatability, and reproducibility inter alia of forensic firearms examinations. It was released to the public in early 2021 and then withdrawn. Since then, portions have been republished, but differences between the original report and the publications are interesting.