---
title: "Examining Ames II Repeatability and Reliability"
bibliography: refs.bib
classoption: table
header-includes: |
  \usepackage{booktabs}
format:
  pdf:
    include-in-header: "preamble.tex"
    keep-tex: true
    number-sections: true
    pdf-engine: xelatex
geometry:
  - left=1in
  - right=1in
  - top=1in
  - bottom=1in
biblio-style: abbrvnat
csl: bluebook-law-review.csl
---




# Introduction

Firearms and toolmark examination involves the visual comparison of a 'known' piece of fired ammunition with a corresponding 'unknown' in order to make a statement about the similarity or dissimilarity of the two items. 
In many cases, the 'known' item is created by test firing a recovered firearm, while the 'unknown' item is recovered from a crime scene. 
In other cases, two recovered items may be compared with the goal of linking separate scenes.
In either case, however, additional assumptions are necessary to move from a claim of visual similarity of the fired ammunition to a claim about the weapon which produced the ammunition. 
Additional information is then required in order to logically connect a recovered weapon to an individual and to connect the individual to the crime scene; this information is contextual and should be carefully managed [@quigleymcbridePracticalToolInformation2022] to ensure that it does not bias the examiner's conclusions. 

The Association of Firearms and Toolmark Examiners (AFTE) Theory of Identification [@nationalinstituteofjusticeAFTETheoryIdentification2023] provides the assumptions necessary to link sources based on comparison of fired ammunition. 
It requires that "the unique surface contours of two toolmarks are in sufficient agreement" in order to make common origin determinations (Principle 1). 
Sufficient agreement consists of the correspondence of random characteristics in the surface of a toolmark that "exceeds the best agreement demonstrated between toolmarks known to have been produced by different tools" while also "consistent with agreement demonstrated by toolmarks known to have been produced by the same tool" (Principle 2).
These benchmarks for similarity are inherently "subjective in nature" and "based on the examiner's training and experience" (Principle 3). 

Following an examination (typically, this consists of visual comparison(s) using a comparison microscope), examiners render a conclusion within the AFTE range of conclusions.
There are four primary conclusions which can be made [@nationalinstituteofjusticeAFTERangeConclusions2023]: 

1. Identification, agreement of a combination of individual characteristics and all discernible class characteristics where the extent of agreement exceeds that which can occur in the comparison of toolmarks made by different tools and is consistent with the agreement demonstrated by toolmarks known to have been produced by the same tool.

2. Inconclusive, which has three subcategories:
  A. Some agreement of individual characteristics and all discernable class characteristics, but insufficient for identification,
  B. Agreement of all discernable class characteristics without agreement or disagreement of individual characteristics due to an absence, insufficiency, or lack of reproducibility,
  C. Agreement of all discernable class characteristics and disagreement of individual characteristics, but insufficient for an elimination.

3. Elimination, a significant disagreement of discernible class characteristics and/or individual characteristics. An elimination based on individual characteristics is more complex. If it can be shown that a firearm has not been subjected to significant use or abuse over the period of time following the questioned shooting, the qualitative aspects of the striations (e.g., fineness, coarseness) it produces on fired bullets should remain the same. A difference in these striations indicates an elimination. Elimination based on individual characteristics requires a detailed knowledge of the history and treatment of the firearm, as well as documentation to support the history. It is the responsibility of the examiner to provide this historical documentation. This type of elimination should be approached with caution. Many experienced examiners have never made such an elimination and the protocols of many laboratories do not allow it.

4. Unsuitable for comparison. This outcome is appropriate for fired bullet fragments that do not bear microscopic marks of value for comparison purposes. Examples include fired bullet fragments, jacket fragments, lead bullet cores, lead fragments, other portions of fragmented, mutilated, or otherwise damaged bullets, and metallic fragments that cannot be identified as having been part of a fired bullet. 

The AFTE Theory of Identification and range of conclusions are used by most examiners within the United States; examiners in Europe and the U.K. often use other methods, such as likelihood ratios, to characterize firearms and toolmark comparisons. 

## Scientific Support for Firearms Examination

The AFTE theory of identification is presumed to be "founded on scientific principles", but as discussed in the 2009 NRC report [@NRCStrengthening2009] as well as the 2008 Ballistic Imaging report [@NRCBallisticImaging2008], there has not been scientific validation of the "degree to which firearms-related toolmarks are unique" [@NRCBallisticImaging2008 pg. 3]. 
Furthermore, the 2009 report goes on to say that the AFTE Theory of Identification, "which is the best guidance available for the field of toolmark identification, does not even consider, let alone address, questions regarding variability, reliability, repeatability, or the number of correlations needed to achieve a given degree of confidence [@NRCStrengthening2009 pg 155].
In addition to this assessment, the 2016 PCAST report [@pcast] goes further, identifying the circular definition of an identification: "an examiner may conclude that two items have a common origin if their marks are in 'sufficient agreement,' where 'sufficient agreement' is defined as the examiner being convinced that the items are extremely unlikely to have a different origin."


### Metrics for Scientific Validity

While the PCAST report states that uniqueness is not required for marks to be useful [@pcast], it clearly requires that the accuracy, repeatability, and reproducibility of a method be understood from empirical studies, and further requires that the studies be designed such that each comparison is independent, with the possibility that an unknown object has no match among the known samples (an "open" study), ideally with a kit of separate, statistically independent comparison problems. 
The importance of using an open-set study design with statistically independent comparison problems has been thoroughly documented [@ourAffidavit2022;@inconclusives;@cuellarMethodologicalProblemsEvery2024].
We will instead focus on PCAST's requirements for a determination that a method is based on valid scientific principles. 

- **reliability** - the method must reach a conclusion which is consistent with ground truth.     
Reliability is evaluated using error rate(s). Multiple rates may be considered: while the overall accuracy rate provides some information, an accurate overall error rate requires that study designs emulate the frequency of same-source and different source comparisons which occur during casework, which is unknown. It is typically more useful to consider separately measures of the probabilities of different types of errors, such as the sensitivity and specificity, or the positive and negative predictive values. A more detailed consideration of the merits of different error rates is provided in @inconclusives and @dorfmanInconclusivesErrorsError2022. 

- **repeatablity** - the same comparison performed using the same instrument (in this case, the examiner) must come to the same conclusion. This calculation does not depend at all on accuracy rates: an examiner who comes to the same wrong conclusion examining the same evidence still has high repeatability, even if they have low accuracy. 

- **reproduciblity** - the same comparison performed using a different instrument (examiner) must come to the same conclusion. This calculation also does not depend on the accuracy of the conclusion; all that is required is that the two examiners agree.


### Reliability and Error Rates {#sec-reliability-error-rates}

PCAST indicates that as of the publication of that report, only one study was sufficient to estimate reliability, and in order for there to be sufficient evidence for foundational validity of toolmark examination, there must be at least two studies conducted by independent groups that are appropriately designed and statistically reliable.
Since 2016, there have been several studies conducted which are open-set, but at present, all studies have common, critical flaws which limit our ability to reliably estimate the error rate of subjective comparisons made by examiners [See @cuellarMethodologicalProblemsEvery2024, which is also under review in Law, Probability, and Risk, and @ourAffidavit2022].
PCAST cited the Ames 1 study as being sufficient to estimate reliability, overlooking concerns with the drop-out rate, nonresponse rate, and sampling methodology present in the study; these concerns have a large effect on the potential estimated error rate, and as a result, I disagree with the PCAST report's evaluation of Ames I as a reliable study, not primarily because of its design, but because of its sampling, execution, analysis, and complete reporting of study results.

In addition to the experimental design and reporting concerns, there are significant concerns with how error rates are calculated in firearms examination error rate studies. 
Most studies consider inconclusive results to be correct, which is in accordance with the AFTE Theory of Identification[^ctsincl], but, scientifically, this is sub-optimal and the subject of considerable debate in both the scientific  [@drorMisUseScientific2020; @drorCannotDecideFine2019; @biedermannAreInconclusiveDecisions2019;@inconclusives;@dorfmanInconclusivesErrorsError2022] and the legal community [@kayeToolmarkComparisonTestimonyReport2022]. 
Under this evaluation scheme, an examiner who turns in an answer sheet with every answer marked as inconclusive would score 100%, even though they provided no information about the similarity (or lack thereof) of the examined evidence. 

[^ctsincl]: CTS used to treat inconclusives as errors, but in 1998 changed to treating inconclusives as correct decisions.
    The error rates dropped from 12% (firearms) and 26% (toolmarks) to approximately 1.4% and 4% respectively.
    UNITED STATES OF AMERICA, v. Joseph MINERD, Defendant., 2002 WL 32995663 (W.D.Pa.)


At this time, only one study (published across several reports and journal articles) [@bajicValidationStudyAccuracy2020;@chumbleyAccuracyRepeatabilityReproducibility2021;@monsonPlanningDesignLogistics2022;@monsonAccuracyComparisonDecisions2023;@monson2023repeatability], hereafter referred to as Ames II, examines the repeatability and reproducibility of visual firearms and toolmark comparisons. 
While there are a significant number of flaws with the Ames II study, it is useful to specifically focus it as the most complete study presently published, with a particular focus on the repeatability and reproducibility of firearms and toolmark examinations as they are conducted under the AFTE guidelines. 
Unfortunately, the published analysis of reproducibility and reliability is flawed[@dorfmanReAnalysisRepeatabilityReproducibility2022 discusses the flaws in the final grant report which was briefly published alongside @bajicValidationStudyAccuracy2020 and then swiftly removed from the web. The analysis was later published in @monson2023repeatability, but has the same flaws with the expected vs. observed accuracy analysis]; here, we will primarily work with the published data. 
The remainder of this assessment uses the data reported in @monsonPlanningDesignLogistics2022, @monsonAccuracyComparisonDecisions2023, and @monson2023repeatability. 


# Reliability, Repeatability, and Reproducibility in the Ames II study

## Overall Study Design
Ames II was designed to answer some of the primary objections raised in the PCAST and NRC reports to error-rate studies in firearms and toolmark analysis. 
The study's design is extremely complicated, going to extreme lengths to vary the number of same-source and different-source comparisons while ensuring that no test kit contained sets which could be combined to gain additional information outside of the experimental design.
While this is not a bad thing, this additional complexity makes it difficult to reconstruct information about the study's conduct; in addition, in the interests of confidentiality, the study authors have not released results which would be necessary to conduct different analyses on the collected data, such as individual result sheets or kits for each round of the study.
In this respect, the study's design is a net negative, as it is not possible to fully understand or assess the study due to the combination of unpublished results and a complicated experimental design. 


### Firearms
The study used Beretta and non-Beretta firearms (Jimenez slides for cartridge case comparisons, Ruger barrels for bullet comparisons). 
Within the Beretta barrels, there were consecutively manufactured clusters taken from early, mid, and late points in the lifetime of the machining components, along with four nonconsecutive and unrelated additions. 
Beretta slides had breech-faces which were primarily consecutively finished, with 4 nonconsecutive and unrelated additions. 
Jimenez slides were consecutively finished (so breech faces were consecutively manufactured, but firing pins and extractors were not).  
Ruger barrels were primarily consecutively manufactured (1-9), with an additional barrel included from sequence 33. 
This design element represents a compromise between including close non-matches (consecutively manufactured components) and a spectrum of non-matches with some breadth in marking features.
While it would be good to have more breadth in manufacturing time (e.g. taking consecutive batches from several months or years of manufacturing runs of a single manufacturing process), as well as additional manufacturers represented, any single study has to balance breadth and depth of representation along any single dimension.
Ideally, similar study designs [^extrabullets] would be conducted by other research groups, using other manufacturers; the results would then be comparable to the results of this study and would provide some assurance that the reliability, repeatability, and reproducibility estimates from Ames II generalize across different types of firearms.

[^extrabullets]: @monsonPlanningDesignLogistics2022 notes that consecutively manufactured Jimenez barrels were used in this study, though the bullets are not included; it seems likely that the authors may conduct another follow-up study that provides more data on bullet comparisons. While replication studies by other groups are required for full scientific validity [@pcast], there is certainly a benefit to replication studies on different firearms as well. 

### Ammunition

@monsonPlanningDesignLogistics2022 reports that only a single type of ammunition (Wolf Polyformance 9mm Luger steel case, with brass Berden primers and 115 grain FMJ bullets) was used in the Ames II study. 
The authors justification for this choice is multi-faceted, but it seems that cost, comparison difficulty, and simplification of the experiment factored into the decision.
As the authors note, steel-case ammunition is less common in casework than brass ammunition and does not mark as well as brass; thus, this simplification in the experimental design has the potential to reduce the generalizability of this study to comparisons common in casework. 
The authors suggest that steel-case ammunition comparisons may have fewer identifying marks, but no data is offered to support this assertion. 
While it is desirable to have studies which include difficult comparisons, at present, there is no way to empirically establish how difficult the comparisons in one study are to those in another study: the fired ammunition used is not made available to other researchers, and 3D microscopy data for each piece of ammunition used in the study is also not released to the community, even though this data would both increase the utility of the study long-term and would allow outside researchers to establish difficulty indices for cross-study comparisons.

### Participants
The primary design flaw with the Ames II study in terms of statistical validity is in the recruitment and attrition rate of participants (as well as in how this was reported). 
@monsonPlanningDesignLogistics2022 describes an initial plan for 300 participants, initial agreement of 256 participants at the beginning of the study, that 173 participants completed at least one round of the 6 rounds of testing, and that 79 participants completed all rounds of testing.
The authors describe the participant attrition and recruitment process in a manner that makes it impossible to reconstruct how many participants were involved at any given stage of the study. 

> "Once the first mailing of specimen packets was distributed and volunteers became fully aware of the amount of work required, many examiners decided to drop out of the study without analyzing the first test packet. Additional examiners withdrew from the study over the course of the data collection period. Other examiners joined the study after it was underway."

This description makes it impossible to reconstruct the number of packets completed in each round of the study, the number of participants who initially dropped out, the total number of participants recruited, and other quantities which are essential to calculate the participant drop-out rate. 
In addition, there is no information given about how the participants who joined during the study were recruited, whether they meaningfully differ from those who were initially recruited, which participants (anonymized) completed each of the 6 rounds of the study, and so-on. 
One of the biggest threats to statistical validity is the participant drop-out rate and the item nonresponse bias; failure to report information which could be used to reconstruct these values (or the full data from which study results are calculated, as is customary in other scientific disciplines) means that this study's reported rates for accuracy, repeatability, and reproducibility are utterly unreliable by scientific standards.
Fundamentally, because these numbers are not reported, it is impossible to adjust observed data to account for nonresponse bias.
Moreover, the authors of this series of papers identify that participants may have dropped out due to concerns over the amount of work. 
An alternate explanation is that as the participants had access to the first test packet, they may have balked at the difficulty of the comparisons in the study. 
If this is the case, it seems reasonable that these participants might have had a higher error rate than those who continued with the study. 
Statistically, the presence of a plausible alternate explanation for an effect of this magnitude is sufficient to question the overall results produced by the study. 

```{r, include = F}
library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(RColorBrewer)
reliability_orig <- read_csv("data/AmesII-reliability.csv")
repeatability_orig <- read_csv("data/AmesII-repeatability.csv", skip = 1, name_repair = make.names)
reproducibility_orig <- read_csv("data/AmesII-reproducibility.csv", skip = 1, name_repair = make.names)


conclusion_levels <- c("ID", "INC-A", "INC-B", "INC-C", "EL", "US", "Other")

res_scale <- c(brewer.pal(5, "RdBu"), "grey", "black")

repeatability <- repeatability_orig |>
  filter(!str_detect(X1st.eval, "Total")) |>
  pivot_longer(ID:Total.US, names_to = "X2nd.eval", values_to = "Count") |>
  filter(!str_detect(X2nd.eval, "Total")) |>
  rename(eval1 = X1st.eval, eval2 = X2nd.eval) |>
  mutate(eval2 = str_replace(eval2, "\\.", "-")) |>
  group_by(Type, Comparison) |>
  mutate(comparison_pct = Count/sum(Count)*100)

repeatability_us <- repeatability |> 
  mutate(suitable1 = eval1 != "US", 
         suitable2 = eval2 != "US", 
         match = (suitable1 == suitable2)) |>
  group_by(suitable1, suitable2, match) |>
  summarize(Count = sum(Count)) |>
  ungroup() |>
  mutate(class = if_else(match & suitable1, "Suitable", if_else(match, "Unsuitable", "Conflict"))) |>
  select(-suitable1, -suitable2, -match) |>
  mutate(rate = Count/sum(Count) * 100) |>
  select(-Count) |>
  pivot_wider(names_from = class, values_from = rate, values_fn = sum)

reproducibility <- reproducibility_orig |>
  filter(!str_detect(X1st.eval, "Total")) |>
  pivot_longer(ID:Total.US, names_to = "X2nd.eval", values_to = "Count") |>
  filter(!str_detect(X2nd.eval, "Total")) |>
  rename(eval1 = X1st.eval, eval2 = X2nd.eval) |>
  mutate(eval2 = str_replace(eval2, "\\.", "-")) |>
  group_by(Type, Comparison) |>
  mutate(comparison_pct = Count/sum(Count)*100)
  
reproducibility_us <- reproducibility |> 
  mutate(suitable1 = eval1 != "US", 
         suitable2 = eval2 != "US", 
         match = (suitable1 == suitable2)) |>
  group_by(suitable1, suitable2, match) |>
  summarize(Count = sum(Count)) |>
  ungroup() |>
  mutate(class = if_else(match & suitable1, "Suitable", if_else(match, "Unsuitable", "Conflict"))) |>
  select(-suitable1, -suitable2, -match) |>
  mutate(rate = Count/sum(Count) * 100) |>
  select(-Count) |>
  pivot_wider(names_from = class, values_from = rate, values_fn = sum)

reliability <- reliability_orig |>
  filter(!str_detect(Eval, "Total")) |>
  group_by(Type, Comparison) |>
  rename(Count = Total) |>
  mutate(Total = sum(Count), comparison_pct = Count/Total*100)

reliability_us <- filter(reliability, Eval == "US")

reliability_csdr <- reliability |>
  filter(!Eval %in% c("US", "Other")) |>
  mutate(csd = (Comparison == "SS" & Eval == "ID") | (Comparison == "DS" & Eval == "EL")) |>
  group_by(Type, Comparison) |>
  mutate(Total = sum(Count)) |>
  ungroup() |>
  group_by(Type, csd) |>
  summarize(Count = sum(Count)) |>
  group_by(Type) |>
  mutate(csdr = Count/sum(Count)*100) |>
  filter(csd) |>
  select(Type, csdr)

reliability_sens_spec <- reliability |>
  filter(!Eval %in% c("US", "Other")) |>
  mutate(error = !((Comparison == "SS" & Eval == "ID") | (Comparison == "DS" & Eval == "EL"))) |>
  group_by(Type, Comparison) |>
  summarize(rate = sum(Count*error)/sum(Count) * 100)
reliability_sens <- filter(reliability_sens_spec, Comparison == "SS")
reliability_spec <- filter(reliability_sens_spec, Comparison == "DS")


reliability_fp_fn <- reliability |>
  filter(!Eval %in% c("US", "Other")) |>
  mutate(error = ((Comparison == "SS" & Eval == "EL") | (Comparison == "DS" & Eval == "ID"))) |>
  group_by(Type, Comparison) |>
  summarize(rate = sum(Count*error)/sum(Count) * 100)
reliability_fn <- filter(reliability_fp_fn, Comparison == "SS")
reliability_fp <- filter(reliability_fp_fn, Comparison == "DS")

reliability_ppv_npv <- reliability |>
  filter(!Eval %in% c("US", "Other") & !str_detect(Eval, "INC")) |>
  mutate(error = ((Comparison == "SS" & Eval == "ID") | (Comparison == "DS" & Eval == "EL"))) |>
  group_by(Type, Eval) |>
  summarize(rate = sum(Count*error)/sum(Count) * 100)
reliability_ppv <- filter(reliability_ppv_npv, Eval == "ID")
reliability_npv <- filter(reliability_ppv_npv, Eval == "EL")
```

```{r, include = F}
first_round <- sprintf("%.2f%%", (1 - 173/256)*100)
first_to_last_round <- sprintf("%.2f%%", (1 - 79/173)*100)
recruited_completed_all <-  sprintf("%.2f%%", (1 - 79/256)*100)
bullet_nonresponse <- sprintf("%.2f%%", (1 - 10020/(256*15*6))*100)
cartridge_nonresponse <- sprintf("%.2f%%", (1 - 10110/(256*15*6))*100)
overall_nonresponse <- sprintf("%.2f%%", (1 - (10020 + 10110)/(256*30*6))*100)


```

If we assume that the 173 participants who completed some portion of the 6 rounds of the study were all recruited at the beginning of the study (which they were not) and completed the first round, the drop-out rate for only the first round of the study would be `{r} first_round`, which is well above the 5-10% drop-out rate which is considered reasonable [@khanShiningLightForensic2023]. 
While this is not a completely accurate estimate of the drop-out rate for the entire study (some of the 173 participants were added after round 1, and the 256 initial recruits does not include these individuals), it is illustrative. 
If we compute the drop-out rate as those who completed at least one round but did not complete all six rounds, then we would get `{r} first_to_last_round`, which is even more alarming (and does not account for those who dropped out after seeing the first test packet). 
Of those who were initially recruited, only `{r} recruited_completed_all` completed all six rounds of the study.
Science relies on shared data; that the authors of this study have not published anonymized data which allows others to calculate these measures precisely is another indication that the study was not conducted in line with scientific best practices. 

In addition to the participant drop-out rate, there are some instances of item nonresponse as well: @monson2023repeatability reports that some examiners did not complete bullet comparisons but did complete cartridge case comparisons. 
Using the information available to us, we can come up with an approximate upper bound for the error estimate by assuming that any participant and item nonresponses would have been incorrect (obviously, this will produce an inflated error estimate, but it is illustrative to show the magnitude of the problem). 
Under this assumption (that all nonresponses would have been incorrect, and that all responses were correct), error rates could be as high as `{r} bullet_nonresponse` for bullets and `{r} cartridge_nonresponse` for cartridge cases (using the total number of sets analyzed in all rounds reported in Table 1 of @monson2023repeatability, compared with 256 participants $\times$ 15 comparisons per kit $\times$ 6 rounds of the study). 
This upper estimate is actually not a complete upper bound: another type of nonresponse is the failure to make a conclusive decision (e.g. one of the 3 levels of inconclusive allowed by the study); due to the way comparisons are reported, it is incredibly difficult to determine an upper bound for the error rate if one counts inconclusives as either omitted answers or as incorrect over all six rounds of the study. 
In any case, without proper measurement of how participants who only completed a portion of the study differ from those who signed up to participate and from those who completed the full study, it is impossible to determine where the error rate for firearms and toolmark comparisons is. 
We know that it is most likely somewhere between 0 and `{r} overall_nonresponse`, but we cannot provide statistically valid assurances for a specific number within this interval. 

## Analysis Methods

The Ames II journal articles and reports present both the tabulated results of the experiment and analyses which attempt to provide context or a basis for statistical inference. 
Unfortunately, the analyses presented in the repeatability and reproducibility paper have been shown to be inappropriate [@dorfmanReAnalysisRepeatabilityReproducibility2022]; the authors had time to address these complaints, which were based on the NIJ final grant report, but chose not to do so and instead published the same analysis in their peer-reviewed articles. 
While there are problems with the expected agreement analysis in @monson2023repeatability, a plain examination of the tabular results is sufficient to demonstrate that firearm and toolmark analysis as performed using  AFTE standards and conclusions has major threats to scientific validity, even ignoring the issues outlined in the previous section that are common to all firearms black-box studies. 
As a result, this assessment will focus solely on simple probability calculations from tabular results reported in the series of Ames II publications. 


## Unsuitable Evidence
Unsuitable evidence, according to the examples provided by the AFTE range of conclusions, is fragmented or severely damaged to the point where it has no value for comparison. 
As even misshapen bullets with no microscopic characteristics can still be weighed to establish class characteristics such as caliber, it seems that unsuitable must be reserved for fragmented, incomplete evidence. 
The authors of the Ames II study describe the procedures used to ensure the fired ammunition was collected with minimal incidental damage - bullets were fired into a water tank and cartridges caught with a net; between study rounds, bullets were cleaned carefully and any debris or markings were removed. 
The authors devote considerable care to describing the process of designing the experiment and collecting the data; surely they would have mentioned any damaged bullets recovered and included in the test kits as evidence of the realism of the study design. 
The lack of any documentation, combined with the general care with which the exemplars for the study were collected, suggest that any fired ammunition in the test kits were not fragmented or damaged.

Each lab has protocols determining how the AFTE range of conclusions is applied, including whether an examiner may eliminate based on individual characteristics. 
Furthermore, each examiner has their own individual standards for each category in the AFTE range. 
As a result, the notion that there is a uniform evaluation process in firearms examination is, ultimately, a fiction.
This does not, of course, preclude estimation of a discipline-wide error rate, but as unsuitable evaluations are  excluded from subsequent error rate calculations in these studies, it is worth considering unsuitable evaluations on their own.
Given the examples of unsuitable evidence in the AFTE range of conclusions, examiners trained to compare minuscule details in fired ammunition should be able to identify whether evidence is too damaged to compare with an incredibly high rate of correspondence; that is, the repeatability and reproduciblity rates should be extremely high. 

@monsonAccuracyComparisonDecisions2023 reports that in the first round, between `{r} sprintf("%0.2f%%", min(reliability_us$comparison_pct))` and `{r} sprintf("%0.2f%%", max(reliability_us$comparison_pct))` of comparisons were labeled as unsuitable.
More interesting is whether examiners agree with their own previous evaluations (repeatability) or the evaluations of other examiners as to the suitability of the evidence for comparison. 
We would expect that unsuitable evidence would be unsuitable even upon further examination; there should be little ambiguity in this categorization compared with the choice to use one of the 3 subcategories of inconclusive, for example. 
However, @monson2023repeatability reports that examiners evaluated suitability differently `{r} sprintf("%0.2f%%", repeatability_us$Conflict)` of the time when examining the same evidence (computed across ground truth and evidence type). 
Examiners should be using the same lab protocol and individual standards for unsuitability determinations, and there should be minimal ambiguity in suitability both due to the experimental design and the severity of the examples of unsuitable evidence in the AFTE range of conclusions.
When different examiners evaluated the same comparisons ("reproducibility"), the rate of disagreement across ground truth and evidence type was `{r} sprintf("%0.2f%%", reproducibility_us$Conflict)`, which is slightly higher than the repeatability estimate, which would be expected given the variability in examiner standards and protocol across labs. 


## Reliability

The AFTE range of conclusions provides categories beyond those corresponding to ground truth, which complicates the calculation of error rates, as described in @sec-reliability-error-rates. 
We can evaluate the reliability of firearms examination using many different metrics:

- Correct Source Decision Rate: The number of comparisons which are correctly identified (for same source) or eliminated (for different source), divided by the total number of comparisons. @monsonAccuracyComparisonDecisions2023 reports the CSDR for bullets and cartridges as `{r} sprintf("%.2f%% and %.2f%%", reliability_csdr$csdr[1], reliability_csdr$csdr[2])`, respectively. Higher values indicate better methods.

- Sensitivity and Specificity:

  - Sensitivity: Ability to correctly identify same-source evidence. Strictly calculated as the number of same-source identifications divided by the total number of same source comparisons (e.g. inconclusives are not consistent with ground truth). Under this definition, the Ames II accuracy study obtained sensitivity rates for bullets and cartridges of `{r} sprintf("%.2f%% and %.2f%%", reliability_sens$rate[1], reliability_sens$rate[2])`, respectively. Higher values are better. 

  - Specificity: Ability to correctly identify different-source evidence, calculated as the number of different source eliminations divided by the total number of different source comparisons. Under this definition, the Ames II accuracy study observed specificity rates for bullets and cartridges of `{r} sprintf("%.2f%% and %.2f%%", reliability_spec$rate[1], reliability_spec$rate[2])`, respectively.  Higher values are better.


- False positive rate (FPR) and False negative rate (FNR):

  - FPR: Probability of making an identification when examining a different-source comparison. Under this definition, @monsonAccuracyComparisonDecisions2023 observed false positive rates for bullets and cartridges of `{r} sprintf("%.2f%% and %.2f%%", reliability_fp$rate[1], reliability_fp$rate[2])`, respectively. Lower values are better. 
  
  - FNR: Probability of making an elimination when examining a same-source comparison. Under this definition, the Ames II accuracy study observed false negative rates for bullets and cartridges of `{r} sprintf("%.2f%% and %.2f%%", reliability_fn$rate[1], reliability_fn$rate[2])`, respectively.  Lower values are better. 
  
- Positive predictive value (PPV) and negative predictive value (NPV):

  - PPV: Probability that when an examiner makes an identification, the comparison is same-source, calculated as the number of same-source identifications divided by the total number of identifications.  Under this definition, examiners in the Ames II accuracy study making an identification have a `{r} sprintf("%.2f%% and %.2f%%", reliability_ppv$rate[1], reliability_ppv$rate[2])` chance that the comparison is same-source for bullets and cartridges, respectively. Higher values are better. 
  
  
  - NPV: Probability that when an examiner makes an elimination, the comparison is different-source, calculated as the number of different-source eliminations divided by the total number of eliminations.  Under this definition, examiners in the Ames II accuracy study making an elimination have a `{r} sprintf("%.2f%% and %.2f%%", reliability_npv$rate[1], reliability_npv$rate[2])` chance that the comparison is different-source comparison for bullets and cartridges, respectively. Higher values are better. 
  
Fundamentally, the discrepancy between the relatively low FPR/FNR and high PPV/NPV, which indicate that the method is reasonably reliable, and the low sensitivity/specificity and correct source decision rate is due to the treatment of inconclusive decisions. 
Examiners argue that inconclusive decisions should not be considered errors, as often, there are insufficient markings available to make a conclusive decision in either direction; in addition, some laboratories do not allow examiners to eliminate based on individual characteristic mismatches, leaving examiners with no choice but to label a comparison as inconclusive-C.
On the other hand, under this philosophy, an examiner could label every comparison as inconclusive and report a 0% error rate, which is clearly ridiculous. 
In medicine, patients with inconclusive tests can be sent back to the lab for a repeat test or a test with a higher resolution, but forensic examiners do not have this option. 
As a result, the discipline must wrestle with the proper way to evaluate the effectiveness of the examination method, and so far, there has been little consensus.
Automatic evidence evaluation algorithms which automatically evaluate bullet and cartridge case scans tend not to have an inconclusive option, and perform with low error rates [@hare2017automatic;@krishnanAdaptingChumbleyScore2019;@zemmelsStudyReproducibilityCongruent2023;@chenFiredBulletSignature2019].
Firearms examination could be considerably more reliable (and could dispense with the inconclusive option) by utilizing these tools to provide quantitative, repeatable, and objective evaluations of evidence.
There is little incentive to adopt these measures, however, until examiners are held to higher standards that encourage conclusive decisions. 


## Repeatability

Repeatability is another measurement of whether a method is scientifically valid: if the same inputs do not produce the same outputs when the same process is performed, this signals that there is a level of randomness in the process. 
Ideally, this randomness is extremely small: a deterministic computer procedure might fail to produce the same results 0.001% of the time, but this is an acceptably small level of inconsistency. 
Ames II is the only study which evaluated repeatability - in a 6-round study, some examiners were sent the same comparisons to evaluate twice, separated by at least one round (e.g. the same comparisons might be sent in rounds 1 and 3, or 2 and 5). 
As the experimenters did not share the full data from the study or even the full design specification that would indicate how, exactly, the replication process was determined, we must be content with analyzing the tabulated data provided in @monson2023repeatability. 

Almost 2% of the repeated evaluations involved a mis-match in suitability assessment, but if we condition on the comparison being rated as suitable for evaluation both times, we can assess whether an examiner comes to the same conclusion given the same evidence (and same instrument, lab protocols, and evaluation criteria).
There are twenty five different possible outcomes when a kit is evaluated twice: each of the five AFTE conclusions could occur during either examination. 
Evaluating what each of these different combinations means can be very confusing, so in the interests of interpretability, we will start off with a calculation that should be favorable to FATM examiners: given that the first examination was an identification, what is the probability that the second is also an identification?
Examiners are better at identifying same-source comparisons than different source comparisons [@inconclusives]; in addition, lab policy rules about class characteristics and eliminations make inconclusives and eliminations a bit more complicated to evaluate.

Repeatability does not consider the correctness of the decision: we are more interested in consistency at this point than accuracy. 
There were 665 evaluations which were both identifications, and 740 first evaluations which were identifications, for same-source bullets; for different-source bullets, there were 2 double-identifications out of 19 which were identifications on the first evaluation. 
If an examiner makes an identification, there is a $100\times \frac{665+2}{740+19}$ = `r sprintf("%.02f%%", 100*(667/759))` chance that a second evaluation will also be an identification. 
That is, more than 10% of the time, the examiner will come to a conclusion other than identification when given another look at evidence (with at least ~6 weeks or more between examinations). 
While other non-identification options include inconclusives (and inconclusive-A, which indicates significant, but insufficient, similarity), this result indicates that examiners individually held thresholds are not even internally consistent. 
Even if examiners have to make a judgment call to decide between identification and inconclusive A, a single examiner's threshold is variable enough that they do not make the same call in over 10% of repeated evaluations.
Existing quantitative evaluation tools, on the other hand, are deterministic - the same version of the tool will come to the same conclusion given the same input, every time. 

```{r}
#| label: tbl-conditional-repeatability
#| tbl-cap: Repeatability of an examiner's second decision, given that the first decision ("Evaluation 1") is specified, across same-source and different-source comparisons. Under the best circumstances (bullets, where the first decision was an identification), repeatability is still below 90%; when Inconclusive-A is the initial decision, repeatability is below 34%. 
#| echo: false
#| message: false
#| warning: false

library(knitr)
repeatability |>
  filter(eval1 != "US" & eval2 != "US") |>
  mutate(eval1 = factor(eval1, conclusion_levels[1:5], ordered = T),
         eval2 = factor(eval2, conclusion_levels[1:5], ordered = T)) |>
  group_by(Type, eval1) |>
  summarize(prop = sprintf("%0.2f%%", sum((eval2 == eval1) * Count) / sum(Count) * 100)) |>
  pivot_wider(names_from = Type, values_from = prop) |>
  rename("Evaluation 1" = eval1) |>
  kable()
```

## Reproducibility

As with repeatability, reproducibility does not consider the correctness of a decision; rather, reproducibility assesses whether different examiners come to the same conclusion given the same evidence.
There is more variability in reproducibility - different examiners may have different microscopes, or work under different lab policies; in addition, each examiner's training and experience are different, and the AFTE standard is explicitly designed around an examiner's experience. 
As a result, we would expect reproducibility to be lower than repeatability, because of the increase in variability when different examiners are included.
As with repeatability, we will take a simple approach to examining Ames II's reported results by considering the second examiner's decision given (statistically, conditioned on) the first examiner's decision. 

```{r}
#| label: tbl-conditional-reproducibility
#| tbl-cap: Reproducibility of an examiner's decision. Given the first examiner's decision ("Evaluation 1"), we calculate the probability that the second examiner agrees with the first, across same-source and different-source comparisons. Under the best circumstances (bullets, where the first decision was an identification), reproducibility is below 85%; when Inconclusive-A is the initial decision, repeatability is below 15% for bullets and even lower for cartridge cases. Reproducibility, unlike repeatability, seems to be different for cartridge cases than bullets.  
#| echo: false
#| message: false
#| warning: false

library(knitr)
reproducibility |>
  filter(eval1 != "US" & eval2 != "US") |>
  mutate(eval1 = factor(eval1, conclusion_levels[1:5], ordered = T),
         eval2 = factor(eval2, conclusion_levels[1:5], ordered = T)) |>
  group_by(Type, eval1) |>
  summarize(prop = sprintf("%0.2f%%", sum((eval2 == eval1) * Count) / sum(Count) * 100)) |>
  pivot_wider(names_from = Type, values_from = prop) |>
  rename("Evaluation 1" = eval1) |>
  kable()
```

# Conclusion

Ultimately, firearms and toolmark analysis has a scientific validity problem. 
Under the best conditions, an examiner makes a different conclusion than they previously made more than 10% of the time; a different examiner would come to a different conclusion under those same circumstances more than 17% of the time. 
While firearms and toolmark examiners like to promote low error rates, these error rates are a mirage - a combination of carefully specified calculations and blindness to issues of nonresponse and drop-out rates which combine with the imprecise AFTE range of conclusions to lead triers of fact to conclude that firearms and toolmark examination is a scientific discipline, similar to DNA comparisons in both error rate and scientific validity. 
It is not. 
While the underlying evidence may be reliable, a straightforward look at the reliability, repeatability, and reproducibility measurements reported in the Ames II publications suggests that as a whole, the current practice of visual examination and subjective evaluation is not using that evidence effectively. 
Quantitative, repeatable, reproducible algorithms that have been validated for use on firearms and toolmark evidence provide the same general benefit (the ability to link evidence to tools) with less variability, more objectiveness, and better scientific validity. 
Moreover, open-source algorithms that are freely available can be tested by external researchers, and their performance evaluated on new datasets, without the time and expense that goes into studies like Ames II. 
While labs would have to invest time into developing protocols for the use of these algorithms and money into digital microscopes, these investments are small compared to the societal costs of errors in either direction made by subjective evaluations of evidence.
Until such time as FATM examination adopts more quantitative, objective, scientific tools, the "best" black-box error rate study in the field serves to demonstrate the staggering lack of scientificity in current practice.
