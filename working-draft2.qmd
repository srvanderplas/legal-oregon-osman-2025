---
title: "Scientific Validity of Firearms and Toolmark Examination: Reliability, Repeatability, and Reproducibility"
bibliography: refs.bib
classoption: table
header-includes: |
  \usepackage{booktabs}
  \usepackage[multiple]{footmisc}
  \usepackage{etoolbox}
    
  \makeatletter
  % Argument: custom value to use locally for \floatingpenalty inside footnotes
  \newcommand*{\mySpecialfootnotes}[1]{%
    \patchcmd{\@footnotetext}{\floatingpenalty\@MM}{\floatingpenalty#1\relax}%
             {}{\errmessage{Couldn't patch \string\@footnotetext}}%
  }
  \makeatother
format:
  pdf:
    include-in-header: "preamble.tex"
    keep-tex: true
    number-sections: true
    pdf-engine: xelatex
geometry:
  - left=1in
  - right=1in
  - top=1in
  - bottom=1in
biblio-style: abbrvnat
csl: bluebook-law-review.csl

filters:
  - quarto
  - fwrappe 
---
 <!-- https://github.com/multimeric/Fwrappe - Quarto extension to wrap figures -->

# Introduction

This amicus brief outlines the fundamental research principles used to evaluate the scientific validity of a method. 
None of the factors discussed in this brief are new: these requirements are common across scientific fields.   
The brief then discusses the application of these principles to the method used by firearms and toolmark examiners. 
Finally, the brief examines the implications of database searches on the reliability of firearms and toolmark examination.

Adhering to the principles of sound research design and statistical analysis is fundamental to any applied science. 
There is no exception for forensic science. 
While the firearms and toolmark field has made strides, current research does not yet support the claims made by the discipline. 
Specifically, existing research studies that evaluated accuracy, reliability, and reproducibility of firearms examination have substantial flaws, described below. 
Our conclusion is that firearms examination has not been demonstrated to be accurate, reliable, or reproducible. 
Error rates for firearms examination (e.g., false positive identifications) are currently unknown, as existing studies are inadequate to establish these error rates.

Issues with experimental design, participant selection, statistical analysis, and the interpretation of estimates pervade the current validation studies.
As just one example, studies count inconclusive responses–those in which the examiner cannot make a definitive conclusion–as effectively correct (i.e., not as errors), which results in misleadingly low reported error rates.^[Inconclusive responses are included in the total number of comparisons performed, the denominator, but not included as errors in the numerator.]  
Treating inconclusive responses as effectively correct results in reported error rates as low as zero percent. 
If inconclusives are instead treated as errors, error rates can be as high as 93%.  
The true error rate is likely between these two extremes, but until more well-designed research is performed, it remains unknown.

While there are encouraging developments in research design, data from a recent study shows an alarming lack of consistency in decisions when the same examiner was presented with the same evidence twice, and when different examiners were presented with the same evidence.
These new data further undermine the claim of a well-developed, scientifically valid method and cannot go unaddressed.

